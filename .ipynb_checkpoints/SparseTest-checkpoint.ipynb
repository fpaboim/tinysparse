{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c475e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.sparsetensor import SparseTensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4ec193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = np.random.randn(1,3).astype(np.float32)\n",
    "U_init = np.random.randn(3,3).astype(np.float32)\n",
    "V_init = np.random.randn(3,3).astype(np.float32)\n",
    "W_init = np.random.randn(3,3).astype(np.float32)\n",
    "m_init = np.random.randn(1,3).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf9e1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.17237905], dtype=float32),\n",
       " <Tensor <GPUBuffer with shape (1, 3)> with grad <GPUBuffer with shape (1, 3)>>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Tensor(x_init)\n",
    "W = Tensor(W_init)\n",
    "m = Tensor(m_init)\n",
    "out = x.dot(W).relu()\n",
    "out = out.logsoftmax()\n",
    "out = out.mul(m).add(m).sum()\n",
    "out.backward()\n",
    "\n",
    "out.cpu().data, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c46e036",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_ell' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18872/36286179.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/HD2/ML/tinygrad/tinygrad/sparsetensor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dense_data, data, idxs, nnzs, ellw, shape, device, requires_grad)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m       \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnzs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mellw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_ell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_ell' is not defined"
     ]
    }
   ],
   "source": [
    "x = Tensor(x_init)\n",
    "W = SparseTensor(W_init)\n",
    "m = Tensor(m_init)\n",
    "out = x.dot(W).relu()\n",
    "out = out.logsoftmax()\n",
    "out = out.mul(m).add(m).sum()\n",
    "out.backward()\n",
    "\n",
    "out.cpu().data, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12efbb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "from time import time\n",
    "import numpy\n",
    "\n",
    "block_size = 16\n",
    "\n",
    "ctx = cl.create_some_context()\n",
    "\n",
    "for dev in ctx.devices:\n",
    "    assert dev.local_mem_size > 0\n",
    "\n",
    "queue = cl.CommandQueue(ctx,\n",
    "        properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "\n",
    "#queue = cl.CommandQueue(ctx)\n",
    "\n",
    "if False:\n",
    "    a_height = 4096\n",
    "    #a_height = 1024\n",
    "    a_width = 2048\n",
    "    #a_width = 256\n",
    "    #b_height == a_width\n",
    "    b_width = a_height\n",
    "\n",
    "elif False:\n",
    "    # like PyCUDA\n",
    "    a_height = 2516\n",
    "    a_width = 1472\n",
    "    b_height = a_width\n",
    "    b_width = 2144\n",
    "\n",
    "else:\n",
    "    # CL SDK\n",
    "    a_width = 128*block_size\n",
    "    a_height = 128*block_size\n",
    "    b_width = 50*block_size\n",
    "    b_height = a_width\n",
    "\n",
    "c_width = b_width\n",
    "c_height = a_height\n",
    "\n",
    "h_a = numpy.random.rand(a_height, a_width).astype(numpy.float32)\n",
    "h_b = numpy.random.rand(b_height, b_width).astype(numpy.float32)\n",
    "h_c = numpy.empty((c_height, c_width)).astype(numpy.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620a0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kernel_params = {\"block_size\": block_size,\n",
    "        \"w_a\":a_width, \"h_a\":a_height, \"w_b\":b_width}\n",
    "\n",
    "prg = cl.Program(ctx, KERNEL_CODE % kernel_params,\n",
    "        ).build(options=\"-cl-mad-enable -cl-fast-relaxed-math\")\n",
    "kernel = prg.matrixMul\n",
    "#print prg.binaries[0]\n",
    "\n",
    "assert a_width % block_size == 0\n",
    "assert a_height % block_size == 0\n",
    "assert b_width % block_size == 0\n",
    "\n",
    "# transfer host -> device -----------------------------------------------------\n",
    "mf = cl.mem_flags\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "d_a_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=h_a)\n",
    "d_b_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=h_b)\n",
    "d_c_buf = cl.Buffer(ctx, mf.WRITE_ONLY, size=h_c.nbytes)\n",
    "\n",
    "push_time = time()-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4b0a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU push+compute+pull total [s]: 0.014005398750305176\n",
      "GPU push [s]: 0.0073621273040771484\n",
      "GPU pull [s]: 0.001420736312866211\n",
      "GPU compute (host-timed) [s]: 0.005222535133361817\n",
      "GPU compute (event-timed) [s]:  0.005093408000000001\n",
      "GFlops/s: 1284.986357895521\n",
      "---------------------------\n",
      "GPU==CPU: False\n",
      "CPU time (s) 0.029859066009521484\n",
      "GPU speedup (with transfer):  2.131968288933641\n",
      "GPU speedup (without transfer):  5.717350912352943\n"
     ]
    }
   ],
   "source": [
    "# warmup ----------------------------------------------------------------------\n",
    "for i in range(5):\n",
    "    event = kernel(queue, h_c.shape, (block_size, block_size), \n",
    "            d_c_buf, d_a_buf, d_b_buf)\n",
    "    event.wait()\n",
    "\n",
    "queue.finish()\n",
    "\n",
    "# actual benchmark ------------------------------------------------------------\n",
    "t1 = time()\n",
    "\n",
    "count = 20\n",
    "for i in range(count):\n",
    "    event = kernel(queue, h_c.shape, (block_size, block_size),\n",
    "            d_c_buf, d_a_buf, d_b_buf)\n",
    "\n",
    "event.wait()\n",
    "\n",
    "gpu_time = (time()-t1)/count\n",
    "\n",
    "# transfer device -> host -----------------------------------------------------\n",
    "t1 = time()\n",
    "cl.enqueue_copy(queue, d_c_buf, h_c)#.wait()\n",
    "pull_time = time()-t1\n",
    "\n",
    "# timing output ---------------------------------------------------------------\n",
    "gpu_total_time = gpu_time+push_time+pull_time\n",
    "\n",
    "print(\"GPU push+compute+pull total [s]:\", gpu_total_time)\n",
    "print(\"GPU push [s]:\", push_time)\n",
    "print(\"GPU pull [s]:\", pull_time)\n",
    "print(\"GPU compute (host-timed) [s]:\", gpu_time)\n",
    "print(\"GPU compute (event-timed) [s]: \", (event.profile.end-event.profile.start)*1e-9)\n",
    "\n",
    "gflop = h_c.size * (a_width * 2.) / (1000**3.)\n",
    "gflops = gflop / gpu_time\n",
    "\n",
    "print(\"GFlops/s:\", gflops)\n",
    "\n",
    "# cpu comparison --------------------------------------------------------------\n",
    "t1 = time()\n",
    "h_c_cpu = numpy.dot(h_a,h_b)\n",
    "cpu_time = time()-t1\n",
    "\n",
    "print(\"---------------------------\")\n",
    "print(\"GPU==CPU:\",numpy.allclose(h_c, h_c_cpu))\n",
    "print(\"CPU time (s)\", cpu_time)\n",
    "\n",
    "print(\"GPU speedup (with transfer): \", cpu_time/gpu_total_time)\n",
    "print(\"GPU speedup (without transfer): \", cpu_time/gpu_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb173b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d0659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyopencl as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0cc978b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 8), (8,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim1 = 4\n",
    "dim2 = 8\n",
    "dim3 = 1\n",
    "\n",
    "sparsity = 0.2\n",
    "\n",
    "a = np.zeros((dim1,dim2))\n",
    "b = np.random.rand(dim2,dim3).flatten().astype(np.float32)\n",
    "\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ecf7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_sparse(mat, sparsity=0.1):\n",
    "    indices = np.array(range(mat.shape[1]))\n",
    "    nrows = int(mat.shape[1]*sparsity)\n",
    "    for row in range(mat.shape[0]):\n",
    "        lim = nrows #+ int(np.random.random()*3)\n",
    "        mat[row][np.random.permutation(indices)[:lim]] = np.random.random(lim)\n",
    "    return mat\n",
    "\n",
    "a = fill_sparse(a, sparsity)\n",
    "#b = fill_sparse(b, sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32f0e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.46130043],\n",
       "       [0.        , 0.        , 0.        , 0.43101542, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.00360091, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.18287891, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d286d740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4183415 , 0.49459058, 0.40111175, 0.0073482 , 0.24470419,\n",
       "       0.3563211 , 0.7307064 , 0.02117306], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b2b88a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00976714, 0.00316719, 0.00150641, 0.06516361])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult = a.dot(b)\n",
    "mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d33049e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67872651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_data(mat):\n",
    "    ellwidth = int(mat.shape[1]/2)\n",
    "    all_rows = []\n",
    "    all_idxs = []\n",
    "    all_nnzs = []\n",
    "    for row in range(mat.shape[0]):\n",
    "        rowdata = []\n",
    "        colidxs = []\n",
    "        all_nnzs.append(0)\n",
    "        for col in range(mat.shape[1]):\n",
    "            val = mat[row][col]\n",
    "            if val != 0:\n",
    "                rowdata.append(val)\n",
    "                colidxs.append(col)\n",
    "                all_nnzs[-1] += 1\n",
    "        rowdata = np.array(rowdata)\n",
    "        rowdata.resize(ellwidth)\n",
    "        all_rows.append(rowdata)\n",
    "        colidxs = np.array(colidxs)\n",
    "        colidxs.resize(ellwidth)\n",
    "        all_idxs.append(colidxs)\n",
    "    all_rows = np.array(all_rows).astype(np.float32).flatten()\n",
    "    all_idxs = np.array(all_idxs).astype(np.uint32).flatten()\n",
    "    all_nnzs = np.array(all_nnzs).astype(np.uint32)\n",
    "    return all_rows, all_idxs, all_nnzs, ellwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c9c6134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.46130043, 0.        , 0.        , 0.        , 0.4310154 ,\n",
       "        0.        , 0.        , 0.        , 0.00360091, 0.        ,\n",
       "        0.        , 0.        , 0.18287891, 0.        , 0.        ,\n",
       "        0.        ], dtype=float32),\n",
       " array([7, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0], dtype=uint32),\n",
       " array([1, 1, 1, 1], dtype=uint32),\n",
       " 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata, acols, annz, ellwa = to_data(a)\n",
    "adata, acols, annz, ellwa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbb29574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acols = acols.astype(np.uint32)\n",
    "#annz = annz.astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "008c1a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.46130043, 0.        , 0.        , 0.        , 0.4310154 ,\n",
       "        0.        , 0.        , 0.        , 0.00360091, 0.        ,\n",
       "        0.        , 0.        , 0.18287891, 0.        , 0.        ,\n",
       "        0.        ], dtype=float32),\n",
       " array([7, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0], dtype=uint32),\n",
       " array([1, 1, 1, 1], dtype=uint32),\n",
       " array([0.4183415 , 0.49459058, 0.40111175, 0.0073482 , 0.24470419,\n",
       "        0.3563211 , 0.7307064 , 0.02117306], dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata, acols, annz, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e741bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = cl.mem_flags\n",
    "adata_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=adata)\n",
    "acols_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=acols)\n",
    "annzs_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=annz)\n",
    "b_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b)\n",
    "\n",
    "prg = cl.Program(ctx, \"\"\"\n",
    "// Every global_id_0 works on a row\n",
    "__kernel void SpMVNaive(__global  float* matData,     // INPUT MATRIX DATA\n",
    "                        __global  uint*  colIdx,\n",
    "                        __global  uint*  rowNnz,\n",
    "                        uint   ellwidth,\n",
    "                        __global  float* vector_x,    // INPUT\n",
    "                        __global  float* vector_y    // OUTPUT\n",
    "                        ) { // LOCAL SHARED BUFFER\n",
    "  uint gid = get_global_id(0);\n",
    "  \n",
    "  \n",
    "  uint nnz    = rowNnz[gid];\n",
    "  float sum = 0;\n",
    "  for (uint i = 0; i < nnz; i++) {\n",
    "    uint index   = gid * ellwidth + i;\n",
    "    uint col     = colIdx[index];\n",
    "    float aval  = matData[index];\n",
    "    float xval  = vector_x[col];\n",
    "    printf(\"aval, xval: %.2f,%.2f:%i-%i \\\\n\", aval, xval, col, index);\n",
    "    sum  += aval * xval;\n",
    "  }\n",
    "  printf(\"SUM/NNZ: %.2f %i \\\\n\", sum, nnz);\n",
    "  vector_y[gid] = sum;\n",
    "}\"\"\").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4c85bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 8), (8,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c740a4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.zeros(a.shape[0]).astype(np.float32)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07aa4d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a72195ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=uint32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ellw = np.array([ellwa]).astype(np.uint32)\n",
    "ellw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98da857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mult = mult.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc7365fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyopencl._cl.NannyEvent at 0x7f786db89ca8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_buf = cl.Buffer(ctx, mf.WRITE_ONLY, mult.nbytes)\n",
    "knl = prg.SpMVNaive  # Use this Kernel object for repeated calls\n",
    "knl(queue, [rows,], None, adata_buf, acols_buf, annzs_buf, ellw, b_buf, res_buf)\n",
    "\n",
    "res_np = np.empty_like(a)\n",
    "cl.enqueue_copy(queue, res, res_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33d8a584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyopencl._cl.Buffer at 0x7f7864092f68>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c197fea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00976714, 0.00316719, 0.00150641, 0.06516361], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b20b9da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00976714, 0.00316719, 0.00150641, 0.06516361], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a6322f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.3283064e-10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res-mult).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91b5654a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asfd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-add7c3dfeb73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masfd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asfd' is not defined"
     ]
    }
   ],
   "source": [
    "asfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04c100ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected maxsize to be an integer or None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-eda02a2bf46b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtinygrad\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/HD2/ML/tinygrad/tinygrad/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtinygrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtinygrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtinygrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/HD2/ML/tinygrad/tinygrad/optim.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sorted in order of increasing complexity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtinygrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/HD2/ML/tinygrad/tinygrad/tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    365\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mpyopencl\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m   \u001b[0;31m# TODO: move this import to require_init_gpu?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtinygrad\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m   \u001b[0m_register_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   \u001b[0mGPU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/HD2/ML/tinygrad/tinygrad/ops_gpu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcl_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_WRITE\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOPY_HOST_PTR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhostbuf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mlru_cache\u001b[0;34m(maxsize, typed)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mmaxsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected maxsize to be an integer or None'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorating_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected maxsize to be an integer or None"
     ]
    }
   ],
   "source": [
    "from tinygrad import SparseTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38f76257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseTensor:\n",
    "    def __init__(self, dense_data, _children=(), _op=''):\n",
    "        data, idxs, nnzs, ellw = to_ell(dense_data)\n",
    "        self.data = data\n",
    "        self.idxs = idxs\n",
    "        self.nnzs = nnzs\n",
    "        self.ellw = ellw\n",
    "        self.shape = dense_data.shape\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
    "        \n",
    "        def _backward(in_grad=0.0):\n",
    "            self.grad = in_grad\n",
    "            return (in_grad,)\n",
    "        self._backward = _backward\n",
    "        \n",
    "\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, SparseTensor) else SparseTensor(other)\n",
    "        out = SparseTensor(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acf077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "KERNEL_CODE = \"\"\"\n",
    "// Thread block size\n",
    "#define BLOCK_SIZE %(block_size)d\n",
    "// Matrix dimensions\n",
    "// (chosen as multiples of the thread block size for simplicity)\n",
    "#define WA %(w_a)d // Matrix A width\n",
    "#define HA %(h_a)d // Matrix A height\n",
    "#define WB %(w_b)d // Matrix B width\n",
    "#define HB WA  // Matrix B height\n",
    "#define WC WB  // Matrix C width\n",
    "#define HC HA  // Matrix C height\n",
    "/*\n",
    " * Copyright 1993-2009 NVIDIA Corporation.  All rights reserved.\n",
    " *\n",
    " * NVIDIA Corporation and its licensors retain all intellectual property and\n",
    " * proprietary rights in and to this software and related documentation.\n",
    " * Any use, reproduction, disclosure, or distribution of this software\n",
    " * and related documentation without an express license agreement from\n",
    " * NVIDIA Corporation is strictly prohibited.\n",
    " *\n",
    " * Please refer to the applicable NVIDIA end user license agreement (EULA)\n",
    " * associated with this source code for terms and conditions that govern\n",
    " * your use of this NVIDIA software.\n",
    " *\n",
    " */\n",
    "/* Matrix multiplication: C = A * B.\n",
    " * Device code.\n",
    " */\n",
    "#define AS(j, i) As[i + j * BLOCK_SIZE]\n",
    "#define BS(j, i) Bs[i + j * BLOCK_SIZE]\n",
    "////////////////////////////////////////////////////////////////////////////////\n",
    "//! Matrix multiplication on the device: C = A * B\n",
    "//! WA is A's width and WB is B's width\n",
    "////////////////////////////////////////////////////////////////////////////////\n",
    "__kernel __attribute__((reqd_work_group_size(16,16,1))) \n",
    "void\n",
    "matrixMul( __global float* C, __global float* A, __global float* B)\n",
    "{\n",
    "    __local float As[BLOCK_SIZE*BLOCK_SIZE];\n",
    "    __local float Bs[BLOCK_SIZE*BLOCK_SIZE];\n",
    "    // Block index\n",
    "    int bx = get_group_id(0);\n",
    "    int by = get_group_id(1);\n",
    "    // Thread index\n",
    "    int tx = get_local_id(0);\n",
    "    int ty = get_local_id(1);\n",
    "    // Index of the first sub-matrix of A processed by the block\n",
    "    int aBegin = WA * BLOCK_SIZE * by;\n",
    "    // Index of the last sub-matrix of A processed by the block\n",
    "    int aEnd   = aBegin + WA - 1;\n",
    "    // Step size used to iterate through the sub-matrices of A\n",
    "    int aStep  = BLOCK_SIZE;\n",
    "    // Index of the first sub-matrix of B processed by the block\n",
    "    int bBegin = BLOCK_SIZE * bx;\n",
    "    // Step size used to iterate through the sub-matrices of B\n",
    "    int bStep  = BLOCK_SIZE * WB;\n",
    "    // Csub is used to store the element of the block sub-matrix\n",
    "    // that is computed by the thread\n",
    "    float Csub = 0.0f;\n",
    "    // Loop over all the sub-matrices of A and B\n",
    "    // required to compute the block sub-matrix\n",
    "    for (int a = aBegin, b = bBegin;\n",
    "             a <= aEnd;\n",
    "             a += aStep, b += bStep) {\n",
    "        // Load the matrices from device memory\n",
    "        // to shared memory; each thread loads\n",
    "        // one element of each matrix\n",
    "        AS(ty, tx) = A[a + WA * ty + tx];\n",
    "        BS(ty, tx) = B[b + WB * ty + tx];\n",
    "        // Synchronize to make sure the matrices are loaded\n",
    "        barrier(CLK_LOCAL_MEM_FENCE);\n",
    "        // Multiply the two matrices together;\n",
    "        // each thread computes one element\n",
    "        // of the block sub-matrix\n",
    "        for (int k = 0; k < BLOCK_SIZE; ++k)\n",
    "            Csub += AS(ty, k) * BS(k, tx);\n",
    "        // Synchronize to make sure that the preceding\n",
    "        // computation is done before loading two new\n",
    "        // sub-matrices of A and B in the next iteration\n",
    "        barrier(CLK_LOCAL_MEM_FENCE);\n",
    "    }\n",
    "    // Write the block sub-matrix to device memory;\n",
    "    // each thread writes one element\n",
    "    C[get_global_id(1) * get_global_size(0) + get_global_id(0)] = Csub;\n",
    "}\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
